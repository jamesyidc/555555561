#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è®¡ç®—2æœˆä»½æ¯å¤©çš„æ³¢å³°æ•°æ®
æŒ‰å¤©ä¿å­˜ç»“æœåˆ° data/coin_change_tracker/wave_peaks/
"""

import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ æºä»£ç ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, '/home/user/webapp/source_code')
from wave_peak_detector import WavePeakDetector

def process_daily_wave_peaks(start_date='20260201', end_date='20260218'):
    """
    å¤„ç†æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ³¢å³°æ•°æ®
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼YYYYMMDD
    """
    data_dir = Path('/home/user/webapp/data/coin_change_tracker')
    output_dir = data_dir / 'wave_peaks'
    output_dir.mkdir(exist_ok=True)
    
    # è½¬æ¢æ—¥æœŸ
    start_dt = datetime.strptime(start_date, '%Y%m%d')
    end_dt = datetime.strptime(end_date, '%Y%m%d')
    
    print('=' * 80)
    print('ğŸ“Š æ‰¹é‡æ³¢å³°æ£€æµ‹åˆ†æ')
    print('=' * 80)
    print(f"\næ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"\nå¼€å§‹å¤„ç†...\n")
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = WavePeakDetector(min_amplitude=35.0, window_minutes=15)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_days = 0
    success_days = 0
    total_peaks = 0
    false_breakout_days = 0
    
    summary_data = []
    
    # éå†æ¯ä¸€å¤©
    current_dt = start_dt
    while current_dt <= end_dt:
        date_str = current_dt.strftime('%Y%m%d')
        data_file = data_dir / f'coin_change_{date_str}.jsonl'
        
        total_days += 1
        
        if not data_file.exists():
            print(f"âš ï¸  {date_str}: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            current_dt += timedelta(days=1)
            continue
        
        print(f"ğŸ“… å¤„ç† {date_str}...", end=' ')
        
        try:
            # åŠ è½½æ•°æ®
            data_records = detector.load_data(str(data_file))
            
            if len(data_records) == 0:
                print("âŒ æ•°æ®ä¸ºç©º")
                current_dt += timedelta(days=1)
                continue
            
            # æ£€æµ‹æ³¢å³°ï¼ˆå…³é—­è°ƒè¯•è¾“å‡ºï¼‰
            import io
            import contextlib
            
            # ä¸´æ—¶æ•è·è¾“å‡º
            f = io.StringIO()
            with contextlib.redirect_stdout(f):
                wave_peaks = detector.detect_wave_peaks(data_records)
                false_breakout = detector.detect_false_breakout(wave_peaks)
            
            # æ„å»ºç»“æœ
            result = {
                'date': date_str,
                'data_points': len(data_records),
                'peaks_count': len(wave_peaks),
                'false_breakout': false_breakout,
                'peaks': wave_peaks,
                'processed_at': datetime.now().isoformat()
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            output_file = output_dir / f'wave_peaks_{date_str}.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            # ç»Ÿè®¡
            success_days += 1
            total_peaks += len(wave_peaks)
            if false_breakout:
                false_breakout_days += 1
            
            # ç®€è¦ä¿¡æ¯
            fb_flag = 'âš ï¸ å‡çªç ´' if false_breakout else ''
            print(f"âœ… {len(wave_peaks)}ä¸ªæ³¢å³° {fb_flag}")
            
            # è®°å½•æ‘˜è¦
            summary_data.append({
                'date': date_str,
                'peaks_count': len(wave_peaks),
                'has_false_breakout': false_breakout is not None,
                'data_points': len(data_records)
            })
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
        
        current_dt += timedelta(days=1)
    
    # ä¿å­˜æ±‡æ€»æ•°æ®
    summary_file = output_dir / 'summary.json'
    summary = {
        'date_range': {
            'start': start_date,
            'end': end_date
        },
        'statistics': {
            'total_days': total_days,
            'success_days': success_days,
            'total_peaks': total_peaks,
            'false_breakout_days': false_breakout_days,
            'avg_peaks_per_day': total_peaks / success_days if success_days > 0 else 0
        },
        'daily_data': summary_data,
        'generated_at': datetime.now().isoformat()
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print('\n' + '=' * 80)
    print('ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡')
    print('=' * 80)
    print(f"\næ€»å¤©æ•°: {total_days}")
    print(f"æˆåŠŸå¤„ç†: {success_days}å¤©")
    print(f"æ£€æµ‹åˆ°æ³¢å³°æ€»æ•°: {total_peaks}ä¸ª")
    print(f"å‡çªç ´å¤©æ•°: {false_breakout_days}å¤©")
    print(f"å¹³å‡æ¯å¤©æ³¢å³°æ•°: {total_peaks / success_days if success_days > 0 else 0:.2f}ä¸ª")
    print(f"\nç»“æœä¿å­˜ä½ç½®:")
    print(f"  è¯¦ç»†æ•°æ®: {output_dir}/wave_peaks_YYYYMMDD.json")
    print(f"  æ±‡æ€»æ•°æ®: {summary_file}")
    print('=' * 80)
    
    return summary

if __name__ == '__main__':
    # å¤„ç†2æœˆä»½çš„æ•°æ®
    summary = process_daily_wave_peaks(start_date='20260201', end_date='20260218')
