#!/usr/bin/env python3
"""
Panic Data Daily Splitter - ææ…Œæ•°æ®æŒ‰æ—¥åˆ†å‰²å™¨
å°† panic_wash_index.jsonl ä¸­çš„æ•°æ®æŒ‰æ—¥æœŸåˆ†å‰²åˆ° panic_daily ç›®å½•
"""
import json
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import pytz

BEIJING_TZ = pytz.timezone('Asia/Shanghai')

def split_panic_data_by_day():
    """å°†panic_wash_index.jsonlæŒ‰æ—¥æœŸåˆ†å‰²åˆ°panic_dailyç›®å½•"""
    
    source_file = Path('/home/user/webapp/data/panic_jsonl/panic_wash_index.jsonl')
    target_dir = Path('/home/user/webapp/data/panic_daily')
    target_dir.mkdir(parents=True, exist_ok=True)
    
    if not source_file.exists():
        print(f"[é”™è¯¯] æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
        return
    
    print(f"[ä¿¡æ¯] å¼€å§‹åˆ†å‰²æ•°æ®: {source_file}")
    
    # æŒ‰æ—¥æœŸåˆ†ç»„æ•°æ®
    daily_data = defaultdict(list)
    total_records = 0
    
    with open(source_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            try:
                record = json.loads(line)
                total_records += 1
                
                # è·å–åŒ—äº¬æ—¶é—´
                beijing_time = record.get('beijing_time', '')
                if not beijing_time:
                    # å¦‚æœæ²¡æœ‰beijing_timeï¼Œä»timestampè½¬æ¢
                    timestamp = record.get('timestamp', 0)
                    if timestamp:
                        dt = datetime.fromtimestamp(timestamp, tz=BEIJING_TZ)
                        beijing_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                
                if beijing_time:
                    # æå–æ—¥æœŸ (YYYY-MM-DD -> YYYYMMDD)
                    date_str = beijing_time.split()[0]  # "2026-02-16"
                    date_key = date_str.replace('-', '')  # "20260216"
                    daily_data[date_key].append(line)
                
            except Exception as e:
                print(f"[è­¦å‘Š] è§£æè®°å½•å¤±è´¥: {e}")
                continue
    
    print(f"[ä¿¡æ¯] å…±è¯»å– {total_records} æ¡è®°å½•")
    print(f"[ä¿¡æ¯] æ¶‰åŠ {len(daily_data)} å¤©æ•°æ®")
    
    # å†™å…¥å„æ—¥æœŸæ–‡ä»¶
    written_files = []
    for date_key, records in sorted(daily_data.items()):
        target_file = target_dir / f'panic_{date_key}.jsonl'
        
        # è¯»å–å·²å­˜åœ¨çš„è®°å½•ï¼ˆå»é‡ï¼‰
        existing_records = set()
        if target_file.exists():
            with open(target_file, 'r', encoding='utf-8') as f:
                for line in f:
                    existing_records.add(line.strip())
        
        # åˆå¹¶æ–°æ—§è®°å½•
        all_records = list(existing_records) + [r for r in records if r not in existing_records]
        new_count = len(all_records) - len(existing_records)
        
        # å†™å…¥æ–‡ä»¶
        with open(target_file, 'w', encoding='utf-8') as f:
            for record in all_records:
                f.write(record + '\n')
        
        written_files.append((date_key, len(all_records), new_count))
        print(f"[å®Œæˆ] {date_key}: {len(all_records)} æ¡è®°å½• (+{new_count} æ–°å¢)")
    
    print(f"\n[æ€»ç»“] æˆåŠŸåˆ†å‰² {len(written_files)} ä¸ªæ—¥æœŸæ–‡ä»¶")
    
    # æ˜¾ç¤ºæœ€è¿‘5å¤©çš„æ•°æ®
    if written_files:
        print("\næœ€è¿‘çš„æ•°æ®æ–‡ä»¶:")
        for date_key, count, new in sorted(written_files)[-5:]:
            print(f"  ğŸ“… {date_key[:4]}-{date_key[4:6]}-{date_key[6:]}: {count} æ¡è®°å½•")

if __name__ == '__main__':
    print("=" * 60)
    print("ææ…Œæ•°æ®æŒ‰æ—¥åˆ†å‰²å™¨")
    print("=" * 60)
    split_panic_data_by_day()
    print("=" * 60)
