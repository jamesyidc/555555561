#!/usr/bin/env python3
"""
å°†panic_dailyç›®å½•çš„å†å²æ•°æ®å¯¼å…¥åˆ°panic_wash_index.jsonl
"""
import json
import os
from datetime import datetime
from pathlib import Path

# æ•°æ®æºç›®å½•
SOURCE_DIR = 'data/panic_daily'
# ç›®æ ‡æ–‡ä»¶
TARGET_FILE = 'data/panic_jsonl/panic_wash_index.jsonl'

def load_existing_data():
    """åŠ è½½ç°æœ‰æ•°æ®"""
    if not os.path.exists(TARGET_FILE):
        return {}
    
    existing = {}
    with open(TARGET_FILE, 'r') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                # ä½¿ç”¨åŒ—äº¬æ—¶é—´ä½œä¸ºkeyå»é‡
                beijing_time = data.get('beijing_time')
                if beijing_time:
                    existing[beijing_time] = data
            except:
                pass
    
    return existing

def convert_to_jsonl_format(record):
    """è½¬æ¢æ•°æ®æ ¼å¼"""
    # ä»panic_dailyæ ¼å¼è½¬æ¢ä¸ºpanic_wash_indexæ ¼å¼
    try:
        timestamp_str = record.get('timestamp')
        data_obj = record.get('data', {})
        beijing_time = data_obj.get('record_time')
        
        # å°†ISOæ ¼å¼æ—¶é—´æˆ³è½¬æ¢ä¸ºæ¯«ç§’çº§æ—¶é—´æˆ³
        timestamp_ms = 0
        if timestamp_str:
            try:
                from dateutil.parser import parse
                dt = parse(timestamp_str)
                timestamp_ms = int(dt.timestamp() * 1000)
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä»beijing_timeè§£æ
                if beijing_time:
                    try:
                        dt = datetime.strptime(beijing_time, '%Y-%m-%d %H:%M:%S')
                        timestamp_ms = int(dt.timestamp() * 1000)
                    except:
                        pass
        
        # æ„å»ºliquidation_data
        liquidation_data = {
            'liquidation_1h': data_obj.get('hour_1_amount', 0),
            'liquidation_24h': data_obj.get('hour_24_amount', 0),
            'liquidation_count_24h': data_obj.get('hour_24_people', 0),
            'open_interest': data_obj.get('total_position', 0)
        }
        
        # æ„å»ºå®Œæ•´è®°å½•
        jsonl_record = {
            'timestamp': timestamp_ms,
            'beijing_time': beijing_time,
            'panic_index': data_obj.get('panic_index', 0),
            'liquidation_data': liquidation_data,
            'level': 'medium'  # é»˜è®¤å€¼
        }
        
        return jsonl_record
    except Exception as e:
        print(f"è½¬æ¢é”™è¯¯: {e}, record: {record.get('timestamp', 'N/A')}")
        return None

def import_from_file(file_path, existing_data):
    """ä»å•ä¸ªæ–‡ä»¶å¯¼å…¥æ•°æ®"""
    new_records = []
    
    try:
        with open(file_path, 'r') as f:
            for line in f:
                try:
                    record = json.loads(line.strip())
                    data_obj = record.get('data', {})
                    beijing_time = data_obj.get('record_time')
                    
                    # è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®
                    if beijing_time in existing_data:
                        continue
                    
                    # è½¬æ¢æ ¼å¼
                    jsonl_record = convert_to_jsonl_format(record)
                    if jsonl_record and jsonl_record['beijing_time']:
                        new_records.append(jsonl_record)
                        existing_data[beijing_time] = jsonl_record
                        
                except Exception as e:
                    continue
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶é”™è¯¯ {file_path}: {e}")
    
    return new_records

def main():
    print("="*60)
    print("å¯¼å…¥å†å²æ•°æ®åˆ° panic_wash_index.jsonl")
    print("="*60)
    
    # 1. åŠ è½½ç°æœ‰æ•°æ®
    print("\n1ï¸âƒ£ åŠ è½½ç°æœ‰æ•°æ®...")
    existing_data = load_existing_data()
    print(f"   ç°æœ‰è®°å½•æ•°: {len(existing_data)}")
    
    # 2. æ‰«æpanic_dailyç›®å½•
    print("\n2ï¸âƒ£ æ‰«æå†å²æ•°æ®æ–‡ä»¶...")
    source_files = sorted(Path(SOURCE_DIR).glob('panic_202602*.jsonl'))
    print(f"   æ‰¾åˆ° {len(source_files)} ä¸ªæ–‡ä»¶")
    
    # 3. å¯¼å…¥æ•°æ®
    print("\n3ï¸âƒ£ å¯¼å…¥æ•°æ®...")
    total_new = 0
    for file_path in source_files:
        date = file_path.stem.replace('panic_', '')
        new_records = import_from_file(file_path, existing_data)
        if new_records:
            print(f"   {date}: å¯¼å…¥ {len(new_records)} æ¡æ–°è®°å½•")
            total_new += len(new_records)
    
    # 4. æ’åºå¹¶å†™å…¥æ–‡ä»¶
    print("\n4ï¸âƒ£ æ’åºå¹¶ä¿å­˜...")
    all_records = sorted(existing_data.values(), key=lambda x: x['beijing_time'])
    
    # å¤‡ä»½åŸæ–‡ä»¶
    if os.path.exists(TARGET_FILE):
        backup_file = f"{TARGET_FILE}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.rename(TARGET_FILE, backup_file)
        print(f"   âœ… å·²å¤‡ä»½åŸæ–‡ä»¶: {backup_file}")
    
    # å†™å…¥æ–°æ–‡ä»¶
    with open(TARGET_FILE, 'w') as f:
        for record in all_records:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')
    
    print(f"   âœ… å·²ä¿å­˜ {len(all_records)} æ¡è®°å½•")
    
    # 5. ç»Ÿè®¡
    print("\n5ï¸âƒ£ ç»Ÿè®¡ç»“æœ")
    print("="*60)
    print(f"åŸæœ‰è®°å½•æ•°: {len(existing_data) - total_new}")
    print(f"æ–°å¢è®°å½•æ•°: {total_new}")
    print(f"æ€»è®°å½•æ•°: {len(all_records)}")
    
    # æ—¥æœŸåˆ†å¸ƒ
    dates = {}
    for record in all_records:
        date = record['beijing_time'].split(' ')[0]
        dates[date] = dates.get(date, 0) + 1
    
    print(f"\nğŸ“… æ—¥æœŸåˆ†å¸ƒ:")
    for date in sorted(dates.keys()):
        print(f"  {date}: {dates[date]}æ¡")
    
    print("\nâœ… å¯¼å…¥å®Œæˆï¼")

if __name__ == '__main__':
    main()
